{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## download necessary files\n",
    "!dx download ./results/ukb_demographics_matched.csv\n",
    "!dx download ./results/fs_derivatives.html\n",
    "\n",
    "!dx download ./results/ukb_vol.csv\n",
    "!dx download ./results/ukb_thickness.csv\n",
    "!dx download ./results/ukb_area.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "now run harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install neuroHarmonize\n",
    "!pip install neuroCombat\n",
    "!pip install statsmodels\n",
    "!pip install nibabel\n",
    "!pip install lxml\n",
    "!pip install --upgrade seaborn\n",
    "# !pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuroHarmonize import harmonizationLearn, harmonizationApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## read the dataframes\n",
    "df_covar = pd.read_csv(\"ukb_demographics_matched.csv\")\n",
    "df_features = pd.read_csv(\"ukb_vol.csv\")\n",
    "df_map = pd.read_html(\"fs_derivatives.html\")[1]\n",
    "\n",
    "## organize df_map\n",
    "df_map = df_map.iloc[1:].reset_index(drop=True)\n",
    "df_map[\"UDI\"] = (\n",
    "                    \"p\"\n",
    "                    + df_map[\"UDI\"].astype(str)\n",
    "                    .str.replace(\"-\", \"_i\", regex=False)\n",
    "                    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "                )\n",
    "df_map = df_map[[\"UDI\", \"Description\"]]\n",
    "\n",
    "## keep necessary columns from features\n",
    "feature_cols = df_map[\"UDI\"].tolist()\n",
    "vol_features = [\"eid\"] + [f for f in feature_cols if f in df_features.columns]\n",
    "df_features = df_features[vol_features]\n",
    "\n",
    "## clean df_covars\n",
    "srt_cols = [\n",
    "            \"speechreceptionthreshold_srt_estimate_left\",\n",
    "            \"speechreceptionthreshold_srt_estimate_right\"\n",
    "            ]\n",
    "df_covar[\"srt\"] = df_covar[srt_cols].sum(axis=1)\n",
    "df_covar.drop(columns=srt_cols, inplace=True)\n",
    "\n",
    "covar_cols = [\"subject_id\",\t\"sex\", \"age\", \"center_v2\", \"center_v3\", \"tin_status\", \"tin_severity\", \"tin_duration\", \"srt\"]\n",
    "df_covar = df_covar[covar_cols]\n",
    "df_covar[\"SITE\"] = df_covar[\"center_v2\"].combine_first(df_covar[\"center_v3\"])\n",
    "df_covar.drop(columns=[\"center_v2\", \"center_v3\"], inplace=True)\n",
    "\n",
    "## merge covar with features\n",
    "df_features.rename(columns={\"eid\" : \"subject_id\"}, inplace=True)\n",
    "df_covar = df_covar.merge(df_features, on=\"subject_id\", how=\"left\")\n",
    "i2_cols = [c for c in df_covar.columns if c.endswith(\"_i2\")]\n",
    "\n",
    "for c in i2_cols:\n",
    "    base = c[:-3]\n",
    "    c_i2 = c\n",
    "    c_i3 = base + \"_i3\"\n",
    "\n",
    "    df_covar[base] = df_covar[c_i2].combine_first(df_covar[c_i3])\n",
    "\n",
    "df = df_covar.drop(columns=[c for c in df_covar.columns if c.endswith((\"_i2\", \"_i3\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## drop some nans in data (happened in fields p27531 p26588 and p26589 for 3 subjects)\n",
    "cols_to_check = df.columns.difference([\"tin_severity\", \"tin_duration\"])\n",
    "df = df.dropna(subset=cols_to_check, how=\"any\")\n",
    "\n",
    "## get controls and create matrixes\n",
    "df_subjects = df[[\"subject_id\", \"tin_status\", \"tin_severity\", \"tin_duration\"]]\n",
    "df_covars = df[[\"age\", \"sex\", \"srt\", \"SITE\"]]\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"p\")]\n",
    "data_matrix = df[feature_cols].to_numpy()\n",
    "\n",
    "df_controls = df.query('tin_status == 0')\n",
    "controls_matrix = df_controls.filter(regex=\"^p\").to_numpy()\n",
    "controls_covars = df_controls[[\"age\", \"sex\", \"srt\", \"SITE\"]]\n",
    "\n",
    "for col in ['sex', 'SITE']:\n",
    "    df_covars[col] = df_covars[col].astype(str)\n",
    "    controls_covars[col] = controls_covars[col].astype(str)\n",
    "\n",
    "## run harmonizing and get model and apply on all\n",
    "hm_model, _ = harmonizationLearn(controls_matrix, controls_covars)\n",
    "my_data_adj = harmonizationApply(data_matrix, df_covars, hm_model)\n",
    "\n",
    "## return back to dataframe\n",
    "df_hm = pd.concat([\n",
    "                        df_subjects.reset_index(drop=True),\n",
    "                        df_covars.reset_index(drop=True),\n",
    "                        pd.DataFrame(my_data_adj, columns=feature_cols)\n",
    "                        ],\n",
    "                        axis=1\n",
    "                        )\n",
    "df_hm.to_csv(\"ukb_vol_harmonized.csv\", index=False)\n",
    "!dx upload ukb_vol_harmonized.csv --dest / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical comparison with different atlases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## read harmonized df\n",
    "df_hm = pd.read_csv('ukb_vol_harmonized.csv')\n",
    "atlas_name = \"hippo_subfields\"\n",
    "covariates = [\"age\", \"sex\", \"srt\"]  \n",
    "correction = \"bonferroni\"\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## ranges from UKB documentation\n",
    "FS_RANGES = {\n",
    "    \"aparc_volume\": {\n",
    "        \"lh\": (27205, 27235),\n",
    "        \"rh\": (27298, 27328),\n",
    "    },\n",
    "    \"aparc_2009_volume\": {\n",
    "        \"lh\": (27477, 27550),\n",
    "        \"rh\": (27699, 27772),\n",
    "    },\n",
    "    \"amygdalar_nuclei\": {\n",
    "        \"lh\": (26600, 26609),\n",
    "        \"rh\": (26610, 26619),\n",
    "    },\n",
    "    \"brainstem\": {\n",
    "        \"both\": (26716, 26720),\n",
    "    },\n",
    "    \"hippo_subfields\": {\n",
    "        \"lh\": (26620, 26641),\n",
    "        \"rh\": (26642, 26663),\n",
    "    },\n",
    "    \"aseg\": {\n",
    "        \"lh\": (26554, 26567),\n",
    "        \"rh\": (26585, 26598),\n",
    "    },\n",
    "    \"thalamic_nuclei\": {\n",
    "        \"lh\": (26664, 26687),\n",
    "        \"rh\": (26688, 26715),\n",
    "    },\n",
    "    \"aparc_thickness\": {\n",
    "        \"lh\": (26756, 26788),\n",
    "        \"rh\": (26857, 26889),\n",
    "    },\n",
    "    \"aparc_area\": {\n",
    "        \"lh\": (26722, 26754),\n",
    "        \"rh\": (26823, 26855),\n",
    "    },\n",
    "    \"aparc_2009s_thickness\": {\n",
    "        \"lh\": (27403, 27476),\n",
    "        \"rh\": (27625, 27698),\n",
    "    },\n",
    "    \"aparc_2009s_area\": {\n",
    "        \"lh\": (27329, 27402),\n",
    "        \"rh\": (27625, 27698),\n",
    "    },\n",
    "}\n",
    "\n",
    "ATLAS_RULES = {\n",
    "    \"thalamic_nuclei\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_thalamus\": [\n",
    "                \"Volume_of_Whole_thalamus_left_hemisphere\",\n",
    "                \"Volume_of_Whole_thalamus_right_hemisphere\",\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"amygdalar_nuclei\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_amygdala\": [\n",
    "                \"Volume_of_Whole_amygdala_left_hemisphere\",\n",
    "                \"Volume_of_Whole_amygdala_right_hemisphere\",\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"hippo_subfields\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_hippocampus\": [\n",
    "                \"Volume_of_Whole_hippocampus_left_hemisphere\",\n",
    "                \"Volume_of_Whole_hippocampus_right_hemisphere\",\n",
    "            ]\n",
    "        },\n",
    "        \"drop\": [\n",
    "            \"Volume_of_Whole_hippocampal_body_left_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_head_left_hemisphere\",\n",
    "            \"Volume_of_Hippocampal_tail_left_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_body_right_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_head_right_hemisphere\",\n",
    "            \"Volume_of_Hippocampal_tail_right_hemisphere\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## define a function to select necessary columns\n",
    "def pcols_from_ranges(ranges):\n",
    "    cols = []\n",
    "    for r in ranges.values():\n",
    "        cols.extend(range(r[0], r[1] + 1))\n",
    "    return [f\"p{i}\" for i in cols]\n",
    "\n",
    "common_cols = list(df_hm.columns[:7])\n",
    "\n",
    "## get the atlas\n",
    "sel_cols = pcols_from_ranges(FS_RANGES[atlas_name])\n",
    "df_atlas = df_hm[\n",
    "                    common_cols + sel_cols\n",
    "                        ]\n",
    "\n",
    "## map names in the UKB mapping to atlas columns\n",
    "# df_map = df_map.iloc[1:].reset_index(drop=True)\n",
    "df_map_1 = df_map.copy()\n",
    "df_map_1[\"UDI\"] = (\n",
    "                    df_map_1[\"UDI\"].astype(str)\n",
    "                    .str.replace(\"-\", \"_i\", regex=False)\n",
    "                    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "                )\n",
    "df_map_1 = df_map_1[[\"UDI\", \"Description\"]]\n",
    "df_map_1 = df_map_1[df_map_1[\"UDI\"].str.endswith(\"_i2\", na=False)].copy()\n",
    "df_map_1[\"UDI\"] = df_map_1[\"UDI\"].str.replace(\"_i2$\", \"\", regex=True)\n",
    "rename_map = dict(zip(df_map_1[\"UDI\"], df_map_1[\"Description\"]))\n",
    "df_atlas.rename(columns=rename_map, inplace=True)\n",
    "df_atlas.columns = (\n",
    "                    df_atlas.columns\n",
    "                    .str.replace(r\"[ \\-\\+]\", \"_\", regex=True)\n",
    "                    .str.replace(r\"[()]\", \"\", regex=True)\n",
    "                    )\n",
    "\n",
    "## some name adjustments for special atlases\n",
    "rules = ATLAS_RULES.get(atlas_name, {})\n",
    "\n",
    "for new_col, cols in rules.get(\"sum\", {}).items():\n",
    "    df_atlas[new_col] = df_atlas[cols].sum(axis=1)\n",
    "    df_atlas.drop(columns=cols, inplace=True)\n",
    "\n",
    "if \"drop\" in rules:\n",
    "    df_atlas.drop(columns=rules[\"drop\"], inplace=True)\n",
    "\n",
    "end = None if atlas_name == \"aparc_2009_volume\" else -1\n",
    "bl_cols = list(df_atlas.columns[7:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## function to run ANOVA + multiple comparison\n",
    "def mass_ancova(\n",
    "    df,\n",
    "    outcome_cols,\n",
    "    group_col,\n",
    "    covariates,\n",
    "    correction=\"fdr_bh\",\n",
    "    alpha=0.05,\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for col in outcome_cols:\n",
    "        formula = f\"{col} ~ {group_col} + \" + \" + \".join(covariates)\n",
    "\n",
    "        try:\n",
    "            model = smf.ols(formula, data=df).fit()\n",
    "\n",
    "            beta = model.params[group_col]\n",
    "            pval = model.pvalues[group_col]\n",
    "            tval = model.tvalues[group_col]\n",
    "\n",
    "            n1 = (df[group_col] == 1).sum()\n",
    "            n0 = (df[group_col] == 0).sum()\n",
    "\n",
    "            mean_tinnitus = df.loc[df[group_col] == 1, col].mean()\n",
    "            mean_control = df.loc[df[group_col] == 0, col].mean()\n",
    "\n",
    "            cohen_d = tval * np.sqrt(1 / n1 + 1 / n0)\n",
    "\n",
    "            results.append({\n",
    "                \"brain_label\": col,\n",
    "                \"beta\": beta,\n",
    "                \"t\": tval,\n",
    "                \"pval\": pval,\n",
    "                \"cohen_d\": cohen_d,\n",
    "                \"n_tinnitus\": n1,\n",
    "                \"n_control\": n0,\n",
    "                \"mean_tinnitus\": mean_tinnitus,\n",
    "                \"mean_control\": mean_control,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"brain_label\": col,\n",
    "                \"beta\": np.nan,\n",
    "                \"t\": np.nan,\n",
    "                \"pval\": np.nan,\n",
    "                \"cohen_d\": np.nan,\n",
    "                \"n_tinnitus\": np.nan,\n",
    "                \"n_control\": np.nan,\n",
    "                \"mean_tinnitus\": np.nan,\n",
    "                \"mean_control\": np.nan,\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "\n",
    "    res = pd.DataFrame(results)\n",
    "\n",
    "    # multiple testing correction\n",
    "    mask = res[\"pval\"].notna()\n",
    "\n",
    "    _, pval_adj, _, _ = multipletests(\n",
    "        res.loc[mask, \"pval\"],\n",
    "        alpha=alpha,\n",
    "        method=correction,\n",
    "    )\n",
    "\n",
    "    res.loc[mask, \"pval_adj\"] = pval_adj\n",
    "    res[\"significant\"] = res[\"pval_adj\"] < alpha\n",
    "\n",
    "    return res\n",
    "\n",
    "df_results = mass_ancova(\n",
    "                        df_atlas,\n",
    "                        bl_cols,\n",
    "                        group_col=\"tin_status\",\n",
    "                        covariates=covariates,\n",
    "                        correction=correction,\n",
    "                        alpha=0.05,\n",
    "                    )\n",
    "df_results.sort_values(by=\"pval\", inplace=True)\n",
    "df_sig = df_results.query('significant == True')\n",
    "df_atlas[\"tin_status\"] = df_atlas[\"tin_status\"].map({1: \"Tinnitus\", 0: \"Control\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting significant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_sig_rois(df, roi):\n",
    "\n",
    "    pal = ['#1f77b4', '#d62728']\n",
    "    order = [\"Control\", \"Tinnitus\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 2.3), layout=\"tight\")\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        y=\"tin_status\",\n",
    "        x=roi,\n",
    "        palette=pal,\n",
    "        width=0.5,\n",
    "        linewidth=1.8,\n",
    "        fill=False,\n",
    "        order=order,\n",
    "        showfliers=False,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    sns.stripplot(\n",
    "        data=df,\n",
    "        y=\"tin_status\",\n",
    "        x=roi,\n",
    "        palette=pal,\n",
    "        linewidth=0,\n",
    "        size=3.5,\n",
    "        edgecolor=None,\n",
    "        jitter=0.25,\n",
    "        alpha=0.1,\n",
    "        order=order,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Clean axes\n",
    "    ax.spines[['top', 'left', 'right']].set_visible(False)\n",
    "    ax.set(ylabel=\"\", xlabel=\"\", yticks=[], title=roi)\n",
    "\n",
    "    handles = [\n",
    "        Patch(facecolor='none', edgecolor=pal[i], linewidth=2, label=order[i])\n",
    "        for i in range(len(pal))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=handles,\n",
    "        title=\"Group\",\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.001, 0.99),\n",
    "        borderaxespad=0,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "rois = df_sig['brain_label'].values.tolist()\n",
    "for roi in rois:\n",
    "    plot_sig_rois(df_atlas, roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting significant correlations (severity + duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mode = \"tin_severity\"\n",
    "covariates = (\"age\", \"sex\", \"srt\")\n",
    "correction = \"fdr_bh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mass_partial_corr(\n",
    "    df,\n",
    "    mode,\n",
    "    feature_cols=None,\n",
    "    covariates=(\"age\", \"sex\", \"srt\"),\n",
    "    correction=\"fdr_bh\",\n",
    "    alpha=0.05,\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    X = sm.add_constant(df[list(covariates)])\n",
    "    y = df[mode]\n",
    "    sev_resid = sm.OLS(y, X, missing=\"drop\").fit().resid\n",
    "\n",
    "    for col in feature_cols:\n",
    "        tmp = df[[col] + list(covariates)].dropna()\n",
    "\n",
    "        if tmp.shape[0] < 10:\n",
    "            results.append({\n",
    "                \"feature\": col,\n",
    "                \"r\": np.nan,\n",
    "                \"pval\": np.nan,\n",
    "                \"n\": tmp.shape[0],\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        X_tmp = sm.add_constant(tmp[list(covariates)])\n",
    "        feat_resid = sm.OLS(tmp[col], X_tmp).fit().resid\n",
    "\n",
    "        r, p = pearsonr(sev_resid.loc[tmp.index], feat_resid)\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"r\": r,\n",
    "            \"pval\": p,\n",
    "            \"n\": tmp.shape[0],\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(results)\n",
    "\n",
    "    # multiple comparison correction\n",
    "    mask = res[\"pval\"].notna()\n",
    "    _, p_adj, _, _ = multipletests(\n",
    "        res.loc[mask, \"pval\"],\n",
    "        method=correction,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    res.loc[mask, \"pval_adj\"] = p_adj\n",
    "    res[\"significant\"] = res[\"pval_adj\"] < alpha\n",
    "\n",
    "    return res.sort_values(\"pval_adj\")\n",
    "\n",
    "\n",
    "df_corr = df_atlas.query('tin_status == \"Tinnitus\"')\n",
    "df_tis = df_corr.dropna(subset=[\"tin_severity\"], how=\"any\")\n",
    "df_tid = df_corr.dropna(subset=[\"tin_duration\"], how=\"any\")\n",
    "\n",
    "mode = \"tin_duration\"\n",
    "if mode == \"tin_severity\":\n",
    "    df = df_tis.copy()\n",
    "elif mode == \"tin_duration\":\n",
    "    df = df_tid.copy()\n",
    "else:\n",
    "    raise ValueError(\"wrong mode.\")\n",
    "\n",
    "feature_cols = list(df.columns[7:])\n",
    "df_corr_results = mass_partial_corr(\n",
    "    df,\n",
    "    mode=mode,\n",
    "    feature_cols=feature_cols,\n",
    "    covariates=covariates,\n",
    "    correction=correction\n",
    ")\n",
    "df_corr_results = df_corr_results.query('significant == True')\n",
    "df_corr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check deviation scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
