{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark initialization and getting access to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata\n",
    "!which java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# spark initialization\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "# find dispensed database name and dataset id\n",
    "dispensed_database = dxpy.find_one_data_object(\n",
    "    classname='database', \n",
    "    name='app*', \n",
    "    folder='/', \n",
    "    name_mode='glob', \n",
    "    describe=True)\n",
    "dispensed_database_name = dispensed_database['describe']['name']\n",
    "\n",
    "dispensed_dataset = dxpy.find_one_data_object(\n",
    "    typename='Dataset', \n",
    "    name='app*.dataset', \n",
    "    folder='/', \n",
    "    name_mode='glob')\n",
    "dispensed_dataset_id = dispensed_dataset['id']\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset['participant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions to get field IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fields_for_id(field_id):\n",
    "    from distutils.version import LooseVersion\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "def field_names_for_id(field_id):\n",
    "    return [f.name for f in fields_for_id(field_id)]\n",
    "\n",
    "def fields_by_title_keyword(keyword):\n",
    "    from distutils.version import LooseVersion\n",
    "    fields = list(participant.find_fields(lambda f: keyword.lower() in f.title.lower()))\n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))\n",
    "\n",
    "def field_names_by_title_keyword(keyword):\n",
    "    return [f.name for f in fields_by_title_keyword(keyword)]\n",
    "\n",
    "def field_titles_by_title_keyword(keyword):\n",
    "    return [f.title for f in fields_by_title_keyword(keyword)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get selected field IDs from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "match_variables = [\n",
    "                    'sex_f31_0_0', 'age_when_attended_assessment_centre_f21003_0_0',\n",
    "                    'uk_biobank_assessment_centre_f54_2_0', 'uk_biobank_assessment_centre_f54_3_0',\n",
    "                    'loud_music_exposure_frequency_f4836_0_0',\n",
    "                    'noisy_workplace_f4825_0_0', 'cochlear_implant_f4792_0_0',\n",
    "                    'hearing_aid_user_f3393_0_0', 'hearing_difficultyproblems_with_background_noise_f2257_0_0',\n",
    "                    'handedness_chiralitylaterality_f1707_0_0', 'alcohol_intake_frequency_f1558_0_0',\n",
    "                    'average_total_household_income_before_tax_f738_0_0',\n",
    "                    'speechreceptionthreshold_srt_estimate_left_f20019_0_0',\n",
    "                    'speechreceptionthreshold_srt_estimate_right_f20021_0_0'\n",
    "                    ]\n",
    "\n",
    "tinnitus_field_ids = ['p4803_i0', 'p4803_i2', 'p4814_i0', 'p28625'] # added extra tinnitus\n",
    "\n",
    "mr_variables = ['p26501_i2', 'p26501_i3'] # to see if they exist\n",
    "\n",
    "demographic_field_ids = [\n",
    "                        'eid',\n",
    "                        'p31', 'p21003_i0',\n",
    "                        'p54_i2', 'p54_i3',\n",
    "                        'p4836_i0',\n",
    "                        'p4825_i0', 'p4792_i0',\n",
    "                        'p3393_i0', 'p2257_i0',\n",
    "                        'p1707_i0', 'p1558_i0',\n",
    "                        'p738_i0',\n",
    "                        'p20019_i0', 'p20021_i0'\n",
    "                        ] + \\\n",
    "                        tinnitus_field_ids + \\\n",
    "                        mr_variables\n",
    "\n",
    "df = participant.retrieve_fields(\n",
    "        names=demographic_field_ids,\n",
    "        engine=dxdata.connect()\n",
    "    )\n",
    "df.toPandas().to_csv('ukb_demographics.csv', index=False)\n",
    "!dx upload ukb_demographics.csv --dest / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have the ukb_demographics.csv, let's prepare columns to be ready for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "## read file\n",
    "df = pd.read_csv(\"ukb_demographics.csv\")\n",
    "\n",
    "## map column names\n",
    "match_variables = [\n",
    "                    'sex_f31_0_0', 'age_when_attended_assessment_centre_f21003_0_0',\n",
    "                    'uk_biobank_assessment_centre_f54_2_0', 'uk_biobank_assessment_centre_f54_3_0',\n",
    "                    'loud_music_exposure_frequency_f4836_0_0',\n",
    "                    'noisy_workplace_f4825_0_0', 'cochlear_implant_f4792_0_0',\n",
    "                    'hearing_aid_user_f3393_0_0', 'hearing_difficultyproblems_with_background_noise_f2257_0_0',\n",
    "                    'handedness_chiralitylaterality_f1707_0_0', 'alcohol_intake_frequency_f1558_0_0',\n",
    "                    'average_total_household_income_before_tax_f738_0_0',\n",
    "                    'speechreceptionthreshold_srt_estimate_left_f20019_0_0',\n",
    "                    'speechreceptionthreshold_srt_estimate_right_f20021_0_0'\n",
    "                    ]\n",
    "tinnitus_field_names = ['tin_status_1', 'tin_status_2', 'tin_severity', 'tin_duration']\n",
    "new_col_names = [\"subject_id\", \"sex\", \"age\", \"center_v2\", \"center_v3\"] + \\\n",
    "            [re.split(r'_f\\d+', s)[0] for s in match_variables[4:]] + \\\n",
    "            tinnitus_field_names + \\\n",
    "            [\"Mean_intensity_v2\", \"Mean_intensity_v3\"]\n",
    "\n",
    "\n",
    "mapping = dict(zip(\n",
    "                list(df.columns),\n",
    "                new_col_names\n",
    "                ))\n",
    "df.rename(columns=mapping, inplace=True)\n",
    "\n",
    "## fixing tinnitus status\n",
    "cols = [\"tin_status_1\", \"tin_status_2\"]\n",
    "mapping = {0: 0, 11: 1}\n",
    "df[cols] = df[cols].map(mapping)\n",
    "df = df.dropna(subset=cols, how=\"all\")\n",
    "df = df[\n",
    "    (df[\"tin_status_1\"] == df[\"tin_status_2\"]) |\n",
    "    (df[\"tin_status_1\"].isna()) |\n",
    "    (df[\"tin_status_2\"].isna())\n",
    "]\n",
    "\n",
    "## HL fix\n",
    "col = 'hearing_difficultyproblems_with_background_noise'\n",
    "df[col] = df[col].map({0: 0, 1: 1})\n",
    "df.dropna(subset=[col], inplace=True)\n",
    "\n",
    "## helper function to clean the df\n",
    "def replace_and_fill_mode(df, column, to_replace=None):\n",
    "    \"\"\"\n",
    "    Replace specific values with NaN and fill missing values with the mode.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas DataFrame\n",
    "        column: column name as string\n",
    "        to_replace: list of values to replace with NaN (default ['Prefer not to answer', 'Do not know'])\n",
    "    \"\"\"\n",
    "    if to_replace is None:\n",
    "        to_replace = ['Prefer not to answer', 'Do not know']\n",
    "    \n",
    "    df[column].replace(to_replace, pd.NA, inplace=True)\n",
    "    df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "\n",
    "# apply the helper function on these columns\n",
    "replace_fill_cols = {\n",
    "    'handedness_chiralitylaterality': ['Prefer not to answer'],\n",
    "    'average_total_household_income_before_tax': ['Prefer not to answer', 'Do not know'],\n",
    "    'loud_music_exposure_frequency': ['Prefer not to answer', 'Do not know'],\n",
    "    'noisy_workplace': ['Prefer not to answer', 'Do not know'],\n",
    "    'cochlear_implant': ['Prefer not to answer'],\n",
    "    'hearing_aid_user': ['Prefer not to answer'],\n",
    "    'hearing_difficultyproblems_with_background_noise': ['Prefer not to answer', 'Do not know'],\n",
    "    'alcohol_intake_frequency': ['Prefer not to answer']\n",
    "}\n",
    "\n",
    "for col, values in replace_fill_cols.items():\n",
    "    replace_and_fill_mode(df, col, to_replace=values)\n",
    "\n",
    "# drop rows with missing critical data\n",
    "df.dropna(subset=[\n",
    "    'speechreceptionthreshold_srt_estimate_left',\n",
    "    'speechreceptionthreshold_srt_estimate_right'\n",
    "    ], how='any', inplace=True)\n",
    "\n",
    "## get only subjects that have MRI at least in one visit\n",
    "df.dropna(subset=[\"Mean_intensity_v2\", \"Mean_intensity_v3\"], how='all', inplace=True)\n",
    "\n",
    "## save\n",
    "df.to_csv(\"ukb_demographics_ready_for_matching.csv\", index=False)\n",
    "!dx upload ukb_demographics_ready_for_matching.csv --dest / "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting distribution\n",
    "%matplotlib qt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_matched = pd.read_csv(\"../data/ukb_demographics_matched.csv\")\n",
    "df_plot = df_matched[[\"age\", \"sex\", \"tin_status\"]]\n",
    "\n",
    "df_plot[\"sex\"] = df_plot[\"sex\"].map({0: \"Female\", 1: \"Male\"})\n",
    "df_plot[\"tin_status\"] = df_plot[\"tin_status\"].map({0: \"Control\", 1: \"Tinnitus\"})\n",
    "\n",
    "pal = [\n",
    "        sns.cubehelix_palette(3, rot=-.2, light=.7).as_hex()[1],\n",
    "        sns.color_palette(\"ch:s=-.2,r=.3\", as_cmap=False).as_hex()[2]\n",
    "]\n",
    "\n",
    "xlim = [5, 80]\n",
    "bw_adjust = 1\n",
    "hues = [\"tin_status\", \"sex\"]\n",
    "pals = []\n",
    "for hue in hues:\n",
    "        g = sns.FacetGrid(\n",
    "                df_plot, hue=hue, aspect=3.5, height=1.6,\n",
    "                palette=pal, xlim=xlim\n",
    "        )\n",
    "\n",
    "        g.map(sns.kdeplot, \"age\", bw_adjust=bw_adjust, clip_on=False, clip=xlim,\n",
    "                fill=True, alpha=0.6, linewidth=1.5)\n",
    "        g.map(sns.kdeplot, \"age\", clip_on=False, color=\"w\", clip=xlim,\n",
    "                lw=1.5, bw_adjust=bw_adjust)\n",
    "        g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
    "\n",
    "        g.figure.subplots_adjust(hspace=.15, top=0.72)\n",
    "        g.set_titles(\"\")\n",
    "        g.add_legend(title=\"\")\n",
    "        g.set(yticks=[], ylabel=\"\", xlabel=r\"age\")\n",
    "        g.despine(bottom=True, left=True)\n",
    "        # g.figure.savefig(saving_dir / f\"{hue}_distribution.pdf\", \n",
    "        #                 format=\"pdf\",       \n",
    "        #                 dpi=300,            \n",
    "        #                 bbox_inches=\"tight\"\n",
    "        #                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on matched dataframe and get full MRI data from UKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load the data\n",
    "df_matched = pd.read_csv(\"ukb_demographics_matched.csv\")\n",
    "subject_ids = df_matched[\"subject_id\"].values.tolist()\n",
    "ids_sql = \",\".join(map(str, subject_ids))\n",
    "\n",
    "def get_field_ids(keyword):\n",
    "    field_ids = [\n",
    "                    str(\n",
    "                    fields_by_title_keyword(fn)[0]\n",
    "                    ).split('\"')[1]\n",
    "                    for fn in field_titles_by_title_keyword(keyword)\n",
    "                    ]\n",
    "    \n",
    "    return field_ids\n",
    "\n",
    "vol_field_ids = ['eid'] + get_field_ids('Volume of')\n",
    "thickness_field_ids = ['eid'] + get_field_ids('Mean thickness of')\n",
    "area_field_ids = ['eid'] + get_field_ids('Area of')\n",
    "\n",
    "print(f\"********* getting volume information ***********\")\n",
    "df = participant.retrieve_fields(\n",
    "    names=vol_field_ids,\n",
    "    engine=dxdata.connect()\n",
    ")\n",
    "\n",
    "df_subset = df.filter(f\"eid IN ({ids_sql})\")\n",
    "df_subset.toPandas().to_csv('ukb_vol.csv', index=False)\n",
    "!dx upload ukb_vol.csv --dest / \n",
    "\n",
    "print(f\"********* getting thickness information ***********\")\n",
    "df = participant.retrieve_fields(\n",
    "    names=thickness_field_ids,\n",
    "    engine=dxdata.connect()\n",
    ")\n",
    "\n",
    "df_subset = df.filter(f\"eid IN ({ids_sql})\")\n",
    "df_subset.toPandas().to_csv('ukb_thickness.csv', index=False)\n",
    "!dx upload ukb_thickness.csv --dest / \n",
    "\n",
    "print(f\"********* getting area information ***********\")\n",
    "df = participant.retrieve_fields(\n",
    "    names=area_field_ids,\n",
    "    engine=dxdata.connect()\n",
    ")\n",
    "\n",
    "df_subset = df.filter(f\"eid IN ({ids_sql})\")\n",
    "df_subset.toPandas().to_csv('ukb_area.csv', index=False)\n",
    "!dx upload ukb_area.csv --dest / \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you have UKB matched file and now run harmonization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neuroHarmonize import harmonizationLearn, harmonizationApply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the dataframes\n",
    "df_covar = pd.read_csv(\"../data/ukb_demographics_matched.csv\")\n",
    "df_features = pd.read_csv(\"../data/ukb_vol.csv\")\n",
    "df_map = pd.read_html(\"../data/fs_derivatives.html\")[1]\n",
    "\n",
    "## organize df_map\n",
    "df_map = df_map.iloc[1:].reset_index(drop=True)\n",
    "df_map[\"UDI\"] = (\n",
    "                    \"p\"\n",
    "                    + df_map[\"UDI\"].astype(str)\n",
    "                    .str.replace(\"-\", \"_i\", regex=False)\n",
    "                    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "                )\n",
    "df_map = df_map[[\"UDI\", \"Description\"]]\n",
    "\n",
    "## keep necessary columns from features\n",
    "feature_cols = df_map[\"UDI\"].tolist()\n",
    "vol_features = [\"eid\"] + [f for f in feature_cols if f in df_features.columns]\n",
    "df_features = df_features[vol_features]\n",
    "\n",
    "## clean df_covars\n",
    "srt_cols = [\n",
    "            \"speechreceptionthreshold_srt_estimate_left\",\n",
    "            \"speechreceptionthreshold_srt_estimate_right\"\n",
    "            ]\n",
    "df_covar[\"srt\"] = df_covar[srt_cols].sum(axis=1)\n",
    "df_covar.drop(columns=srt_cols, inplace=True)\n",
    "\n",
    "covar_cols = [\"subject_id\",\t\"sex\", \"age\", \"center_v2\", \"center_v3\", \"tin_status\", \"tin_severity\", \"tin_duration\", \"srt\"]\n",
    "df_covar = df_covar[covar_cols]\n",
    "df_covar[\"SITE\"] = df_covar[\"center_v2\"].combine_first(df_covar[\"center_v3\"])\n",
    "df_covar.drop(columns=[\"center_v2\", \"center_v3\"], inplace=True)\n",
    "\n",
    "## merge covar with features\n",
    "df_features.rename(columns={\"eid\" : \"subject_id\"}, inplace=True)\n",
    "df_covar = df_covar.merge(df_features, on=\"subject_id\", how=\"left\")\n",
    "i2_cols = [c for c in df_covar.columns if c.endswith(\"_i2\")]\n",
    "\n",
    "for c in i2_cols:\n",
    "    base = c[:-3]\n",
    "    c_i2 = c\n",
    "    c_i3 = base + \"_i3\"\n",
    "\n",
    "    df_covar[base] = df_covar[c_i2].combine_first(df_covar[c_i3])\n",
    "\n",
    "df = df_covar.drop(columns=[c for c in df_covar.columns if c.endswith((\"_i2\", \"_i3\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get controls and create matrixes\n",
    "df_subjects = df[[\"subject_id\", \"tin_status\", \"tin_severity\", \"tin_duration\"]]\n",
    "df_covars = df[[\"age\", \"sex\", \"srt\", \"SITE\"]]\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"p\")]\n",
    "data_matrix = df[feature_cols].to_numpy()\n",
    "\n",
    "df_controls = df.query('tin_status == 0')\n",
    "controls_matrix = df_controls.filter(regex=\"^p\").to_numpy()\n",
    "controls_covars = df_controls[[\"age\", \"sex\", \"srt\", \"SITE\"]]\n",
    "\n",
    "## run harmonizing and get model and apply on all\n",
    "hm_model, _ = harmonizationLearn(controls_matrix, controls_covars)\n",
    "my_data_adj = harmonizationApply(data_matrix, df_covars, hm_model)\n",
    "\n",
    "## return back to dataframe\n",
    "df_hm = pd.concat([\n",
    "                        df_subjects,\n",
    "                        df_covars,\n",
    "                        pd.DataFrame(my_data_adj, columns=feature_cols)\n",
    "                        ],\n",
    "                        axis=1\n",
    "                        )\n",
    "df_hm.to_csv(\"../data/ukb_vol_harmonized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical comparison with different atlases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read harmonized df\n",
    "df_hm = pd.read_csv('../data/ukb_vol_harmonized.csv')\n",
    "atlas_name = \"amygdalar_nuclei\"\n",
    "covariates = [\"age\", \"sex\", \"srt\"]  \n",
    "correction = \"bonferroni\"\n",
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define FS ukb ids and atlases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ranges from UKB documentation\n",
    "FS_RANGES = {\n",
    "    \"aparc_volume\": {\n",
    "        \"lh\": (27205, 27235),\n",
    "        \"rh\": (27298, 27328),\n",
    "    },\n",
    "    \"aparc_2009_volume\": {\n",
    "        \"lh\": (27477, 27550),\n",
    "        \"rh\": (27699, 27772),\n",
    "    },\n",
    "    \"amygdalar_nuclei\": {\n",
    "        \"lh\": (26600, 26609),\n",
    "        \"rh\": (26610, 26619),\n",
    "    },\n",
    "    \"brainstem\": {\n",
    "        \"both\": (26716, 26720),\n",
    "    },\n",
    "    \"hippo_subfields\": {\n",
    "        \"lh\": (26620, 26641),\n",
    "        \"rh\": (26642, 26663),\n",
    "    },\n",
    "    \"aseg\": {\n",
    "        \"lh\": (26554, 26567),\n",
    "        \"rh\": (26585, 26598),\n",
    "    },\n",
    "    \"thalamic_nuclei\": {\n",
    "        \"lh\": (26664, 26687),\n",
    "        \"rh\": (26688, 26715),\n",
    "    },\n",
    "    \"aparc_thickness\": {\n",
    "        \"lh\": (26756, 26788),\n",
    "        \"rh\": (26857, 26889),\n",
    "    },\n",
    "    \"aparc_area\": {\n",
    "        \"lh\": (26722, 26754),\n",
    "        \"rh\": (26823, 26855),\n",
    "    },\n",
    "    \"aparc_2009s_thickness\": {\n",
    "        \"lh\": (27403, 27476),\n",
    "        \"rh\": (27625, 27698),\n",
    "    },\n",
    "    \"aparc_2009s_area\": {\n",
    "        \"lh\": (27329, 27402),\n",
    "        \"rh\": (27625, 27698),\n",
    "    },\n",
    "}\n",
    "\n",
    "ATLAS_RULES = {\n",
    "    \"thalamic_nuclei\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_thalamus\": [\n",
    "                \"Volume_of_Whole_thalamus_left_hemisphere\",\n",
    "                \"Volume_of_Whole_thalamus_right_hemisphere\",\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"amygdalar_nuclei\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_amygdala\": [\n",
    "                \"Volume_of_Whole_amygdala_left_hemisphere\",\n",
    "                \"Volume_of_Whole_amygdala_right_hemisphere\",\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"hippo_subfields\": {\n",
    "        \"sum\": {\n",
    "            \"Volume_of_Whole_hippocampus\": [\n",
    "                \"Volume_of_Whole_hippocampus_left_hemisphere\",\n",
    "                \"Volume_of_Whole_hippocampus_right_hemisphere\",\n",
    "            ]\n",
    "        },\n",
    "        \"drop\": [\n",
    "            \"Volume_of_Whole_hippocampal_body_left_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_head_left_hemisphere\",\n",
    "            \"Volume_of_Hippocampal_tail_left_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_body_right_hemisphere\",\n",
    "            \"Volume_of_Whole_hippocampal_head_right_hemisphere\",\n",
    "            \"Volume_of_Hippocampal_tail_right_hemisphere\",\n",
    "        ],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function to select necessary columns\n",
    "def pcols_from_ranges(ranges):\n",
    "    cols = []\n",
    "    for r in ranges.values():\n",
    "        cols.extend(range(r[0], r[1] + 1))\n",
    "    return [f\"p{i}\" for i in cols]\n",
    "\n",
    "common_cols = list(df_hm.columns[:7])\n",
    "\n",
    "## get the atlas\n",
    "sel_cols = pcols_from_ranges(FS_RANGES[atlas_name])\n",
    "df_atlas = df_hm[\n",
    "                    common_cols + sel_cols\n",
    "                        ]\n",
    "\n",
    "## map names in the UKB mapping to atlas columns\n",
    "df_map = pd.read_html(\"../data/fs_derivatives.html\")[1]\n",
    "df_map = df_map.iloc[1:].reset_index(drop=True)\n",
    "df_map[\"UDI\"] = (\n",
    "                    \"p\"\n",
    "                    + df_map[\"UDI\"].astype(str)\n",
    "                    .str.replace(\"-\", \"_i\", regex=False)\n",
    "                    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "                )\n",
    "df_map = df_map[[\"UDI\", \"Description\"]]\n",
    "df_map = df_map[df_map[\"UDI\"].str.endswith(\"_i2\", na=False)].copy()\n",
    "df_map[\"UDI\"] = df_map[\"UDI\"].str.replace(\"_i2$\", \"\", regex=True)\n",
    "rename_map = dict(zip(df_map[\"UDI\"], df_map[\"Description\"]))\n",
    "df_atlas.rename(columns=rename_map, inplace=True)\n",
    "df_atlas.columns = (\n",
    "                    df_atlas.columns\n",
    "                    .str.replace(r\"[ \\-\\+]\", \"_\", regex=True)\n",
    "                    .str.replace(r\"[()]\", \"\", regex=True)\n",
    "                    )\n",
    "\n",
    "## some name adjustments for special atlases\n",
    "rules = ATLAS_RULES.get(atlas_name, {})\n",
    "\n",
    "for new_col, cols in rules.get(\"sum\", {}).items():\n",
    "    df_atlas[new_col] = df_atlas[cols].sum(axis=1)\n",
    "    df_atlas.drop(columns=cols, inplace=True)\n",
    "\n",
    "if \"drop\" in rules:\n",
    "    df_atlas.drop(columns=rules[\"drop\"], inplace=True)\n",
    "\n",
    "end = None if atlas_name == \"aparc_2009_volume\" else -1\n",
    "bl_cols = list(df_atlas.columns[7:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to run ANOVA + multiple comparison\n",
    "def mass_ancova(\n",
    "    df,\n",
    "    outcome_cols,\n",
    "    group_col,\n",
    "    covariates,\n",
    "    correction=\"fdr_bh\",\n",
    "    alpha=0.05,\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    for col in outcome_cols:\n",
    "        formula = f\"{col} ~ {group_col} + \" + \" + \".join(covariates)\n",
    "\n",
    "        try:\n",
    "            model = smf.ols(formula, data=df).fit()\n",
    "\n",
    "            beta = model.params[group_col]\n",
    "            pval = model.pvalues[group_col]\n",
    "            tval = model.tvalues[group_col]\n",
    "\n",
    "            n1 = (df[group_col] == 1).sum()\n",
    "            n0 = (df[group_col] == 0).sum()\n",
    "\n",
    "            mean_tinnitus = df.loc[df[group_col] == 1, col].mean()\n",
    "            mean_control = df.loc[df[group_col] == 0, col].mean()\n",
    "\n",
    "            cohen_d = tval * np.sqrt(1 / n1 + 1 / n0)\n",
    "\n",
    "            results.append({\n",
    "                \"brain_label\": col,\n",
    "                \"beta\": beta,\n",
    "                \"t\": tval,\n",
    "                \"pval\": pval,\n",
    "                \"cohen_d\": cohen_d,\n",
    "                \"n_tinnitus\": n1,\n",
    "                \"n_control\": n0,\n",
    "                \"mean_tinnitus\": mean_tinnitus,\n",
    "                \"mean_control\": mean_control,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"brain_label\": col,\n",
    "                \"beta\": np.nan,\n",
    "                \"t\": np.nan,\n",
    "                \"pval\": np.nan,\n",
    "                \"cohen_d\": np.nan,\n",
    "                \"n_tinnitus\": np.nan,\n",
    "                \"n_control\": np.nan,\n",
    "                \"mean_tinnitus\": np.nan,\n",
    "                \"mean_control\": np.nan,\n",
    "                \"error\": str(e),\n",
    "            })\n",
    "\n",
    "    res = pd.DataFrame(results)\n",
    "\n",
    "    # multiple testing correction\n",
    "    mask = res[\"pval\"].notna()\n",
    "\n",
    "    _, pval_adj, _, _ = multipletests(\n",
    "        res.loc[mask, \"pval\"],\n",
    "        alpha=alpha,\n",
    "        method=correction,\n",
    "    )\n",
    "\n",
    "    res.loc[mask, \"pval_adj\"] = pval_adj\n",
    "    res[\"significant\"] = res[\"pval_adj\"] < alpha\n",
    "\n",
    "    return res\n",
    "\n",
    "df_results = mass_ancova(\n",
    "                        df_atlas,\n",
    "                        bl_cols,\n",
    "                        group_col=\"tin_status\",\n",
    "                        covariates=covariates,\n",
    "                        correction=correction,\n",
    "                        alpha=0.05,\n",
    "                    )\n",
    "df_results.sort_values(by=\"pval\", inplace=True)\n",
    "df_sig = df_results.query('significant == True')\n",
    "df_atlas[\"tin_status\"] = df_atlas[\"tin_status\"].map({1: \"Tinnitus\", 0: \"Control\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting significant results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sig_rois(df, roi):\n",
    "\n",
    "    pal = ['#1f77b4', '#d62728']\n",
    "    order = [\"Control\", \"Tinnitus\"]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 2.3), layout=\"tight\")\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=df,\n",
    "        y=\"tin_status\",\n",
    "        x=roi,\n",
    "        palette=pal,\n",
    "        width=0.5,\n",
    "        linewidth=1.8,\n",
    "        fill=False,\n",
    "        order=order,\n",
    "        showfliers=False,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    sns.stripplot(\n",
    "        data=df,\n",
    "        y=\"tin_status\",\n",
    "        x=roi,\n",
    "        palette=pal,\n",
    "        linewidth=0,\n",
    "        size=3.5,\n",
    "        edgecolor=None,\n",
    "        jitter=0.25,\n",
    "        alpha=0.1,\n",
    "        order=order,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    # Clean axes\n",
    "    ax.spines[['top', 'left', 'right']].set_visible(False)\n",
    "    ax.set(ylabel=\"\", xlabel=\"\", yticks=[], title=roi)\n",
    "\n",
    "    handles = [\n",
    "        Patch(facecolor='none', edgecolor=pal[i], linewidth=2, label=order[i])\n",
    "        for i in range(len(pal))\n",
    "    ]\n",
    "    ax.legend(\n",
    "        handles=handles,\n",
    "        title=\"Group\",\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.001, 0.99),\n",
    "        borderaxespad=0,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "rois = df_sig['brain_label'].values.tolist()\n",
    "for roi in rois:\n",
    "    plot_sig_rois(df_atlas, roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting significant correlations (severity + duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"tin_duration\"\n",
    "covariates = (\"age\", \"sex\", \"srt\")\n",
    "correction = \"fdr_bh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass_partial_corr(\n",
    "    df,\n",
    "    mode,\n",
    "    feature_cols=None,\n",
    "    covariates=(\"age\", \"sex\", \"srt\"),\n",
    "    correction=\"fdr_bh\",\n",
    "    alpha=0.05,\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    X = sm.add_constant(df[list(covariates)])\n",
    "    y = df[mode]\n",
    "    sev_resid = sm.OLS(y, X, missing=\"drop\").fit().resid\n",
    "\n",
    "    for col in feature_cols:\n",
    "        tmp = df[[col] + list(covariates)].dropna()\n",
    "\n",
    "        if tmp.shape[0] < 10:\n",
    "            results.append({\n",
    "                \"feature\": col,\n",
    "                \"r\": np.nan,\n",
    "                \"pval\": np.nan,\n",
    "                \"n\": tmp.shape[0],\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        X_tmp = sm.add_constant(tmp[list(covariates)])\n",
    "        feat_resid = sm.OLS(tmp[col], X_tmp).fit().resid\n",
    "\n",
    "        r, p = pearsonr(sev_resid.loc[tmp.index], feat_resid)\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"r\": r,\n",
    "            \"pval\": p,\n",
    "            \"n\": tmp.shape[0],\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(results)\n",
    "\n",
    "    # multiple comparison correction\n",
    "    mask = res[\"pval\"].notna()\n",
    "    _, p_adj, _, _ = multipletests(\n",
    "        res.loc[mask, \"pval\"],\n",
    "        method=correction,\n",
    "        alpha=alpha\n",
    "    )\n",
    "\n",
    "    res.loc[mask, \"pval_adj\"] = p_adj\n",
    "    res[\"significant\"] = res[\"pval_adj\"] < alpha\n",
    "\n",
    "    return res.sort_values(\"pval_adj\")\n",
    "\n",
    "\n",
    "df_corr = df_atlas.query('tin_status == \"Tinnitus\"')\n",
    "df_tis = df_corr.dropna(subset=[\"tin_severity\"], how=\"any\")\n",
    "df_tid = df_corr.dropna(subset=[\"tin_duration\"], how=\"any\")\n",
    "\n",
    "mode = \"tin_duration\"\n",
    "if mode == \"tin_severity\":\n",
    "    df = df_tis.copy()\n",
    "elif mode == \"tin_duration\":\n",
    "    df = df_tid.copy()\n",
    "else:\n",
    "    raise ValueError(\"wrong mode.\")\n",
    "\n",
    "feature_cols = list(df.columns[7:])\n",
    "df_corr_results = mass_partial_corr(\n",
    "    df,\n",
    "    mode=mode,\n",
    "    feature_cols=feature_cols,\n",
    "    covariates=covariates,\n",
    "    correction=correction\n",
    ")\n",
    "df_corr_results = df_corr_results.query('significant == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check subjects who changed tinnitus state\n",
    "# maybe try adding TIV \n",
    "# thickness and area"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
